PyTorch vision transformer implementations of the ViT models described in articles by A. Dosovitskiy et al. and S. Lee, S. Lee and B. C. Song.

Model References
[1] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2021.
[2] S. Lee, S. Lee and B. C. Song, "Improving Vision Transformers to Learn Small-Size Dataset From Scratch," in IEEE Access, vol. 10, pp. 123212-123224, 2022, doi: 10.1109
